{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 5\n",
    "Designed by Ben Usman, Sarah Adel Bargal, and Brian Kulis, with help from Kun He and Kate Saenko.\n",
    "\n",
    "This assignment will introduce you to:\n",
    "\n",
    "1. implementation of RNN and LSTM\n",
    "2. training your implemented LSTM\n",
    "3. deriving multi-modal Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: RNN Example\n",
    "\n",
    "(10 points)\n",
    "\n",
    "In this example we train an RNN to infer the parameters of a simple dynamical system.\n",
    "First, we simulate a dynamical system with known parameters (random numbers), and use it to generate outputs. Then, we train an RNN on the generated datapoints, attempting to infer the original parameters.\n",
    "You are expected to run and study the provided code, which will be helpful for the second part (implementing LSTM).\n",
    "\n",
    "1. We define a discretized [dynamical system](https://en.wikipedia.org/wiki/Dynamical_system).\n",
    "At each discrete time $t$, the system observes input $x_t$. The system maintains some \"state\" $h_t$, which will be updated over time.\n",
    "Specifically, the states are updated by the following rule: $h_t = \\max(0, 1-(Wx_t+h_{t-1}))$, where $W$ is a parameter matrix.\n",
    "2. In this example, the input data $\\{x_t\\}$ is randomly generated, and the weight matrix $W$ is also drawn randomly. The system starts from an initial hidden state $h_0$, and runs for $t=1,\\ldots,T$.\n",
    "3. Given the sequence of states $\\{h_1,\\ldots,h_T\\}$, we would like to infer the weights $W$.\n",
    "\n",
    "To start with, the following code segment generates and displays the data.\n",
    "Refer to `data_generator.py` for details of data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we present sample data generated \n",
    "import numpy as np\n",
    "import data_generator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(h_0, w), x, h = data_generator._build_rnn_testdata_matrix()\n",
    "\n",
    "print('variable name, shape, min, max: ')\n",
    "for v, name in zip([h_0, w, x], ['h0', 'w', 'x']):\n",
    "    print(name, v.shape, np.min(v), np.max(v))\n",
    "norm_x_t = np.sum(x*2, axis=1)\n",
    "plt.title('||x(t)||')\n",
    "plt.plot(np.arange(norm_x_t.shape[0]), norm_x_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model the dynamical system with an RNN. We would like the state generated by the RNN to match the actual observed, and we use L2 loss for this purpose.\n",
    "This RNN is a *regression* model since it outputs real values.\n",
    "\n",
    "Below, `build_rnn_regression_model()` gives the model definition, and `train_rnn_with_noise()` generates a batch of data and then runs training on it. \n",
    "We corrupt the data generation process with noise, and let the dimensionality of the state $h$ be a free parameter. Therefore, the training function takes two input arguments: `noise_level` and `n_hidden_dim`.   You will later see how varying them affects reconstruction quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "\n",
    "#############################################################################\n",
    "# RNN model graph\n",
    "def build_rnn_regression_model(shape):\n",
    "    # shape is dict with keys:\n",
    "    # n_steps_per_batch, n_hidden_dim, n_input_dim\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # inputs to the dynamical system\n",
    "        X = tf.placeholder(tf.float32,\n",
    "                           [None, shape['n_steps_per_batch'], shape['n_input_dim']])\n",
    "        # observed state from the dynamical system\n",
    "        y = tf.placeholder(tf.float32, [None, shape['n_hidden_dim']])\n",
    "        \n",
    "        with tf.variable_scope('weights'):\n",
    "            # weight matrix\n",
    "            w = tf.get_variable('w', [shape['n_input_dim'], shape['n_hidden_dim']])\n",
    "            # initial state\n",
    "            h_0 = tf.get_variable('h_0', [shape['n_hidden_dim']])\n",
    "            \n",
    "        # for t = 1 to T, update state \n",
    "        h_t = h_0\n",
    "        for t in range(shape['n_steps_per_batch']):\n",
    "            x_t = X[:, t, :]\n",
    "            h_t = tf.maximum(0.0, 1 - (tf.matmul(x_t, w) + h_t))\n",
    "        \n",
    "        # loss: L2\n",
    "        loss = tf.nn.l2_loss(h_t - y, name='loss')\n",
    "        train_op = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "        summ = tf.summary.scalar('loss_sum_%dd' % shape['n_hidden_dim'], loss)\n",
    "        \n",
    "    return {'inputs': [X, y], 'loss': loss, 'train_op': train_op, 'summ': summ,\n",
    "            'weights': {'w': w, 'h_0': h_0}, 'graph': g}\n",
    "\n",
    "#############################################################################\n",
    "# Main train loop for an RNN regression model\n",
    "# \n",
    "# This takes synthetic data generated by data_generator.build_dataset()\n",
    "# the weight matrix W is then inferred with back-prop \n",
    "def train_rnn_with_noise(noise_level, n_hidden_dim):\n",
    "    # generate data\n",
    "    shapes = dict(n_hidden_dim=n_hidden_dim, n_input_dim=15, n_steps_per_batch=100)\n",
    "    rnn_dataset = data_generator.build_dataset('rnn', noise=noise_level, **shapes)\n",
    "    (h0_true, w_true), batched_data = rnn_dataset  # \"true\" weights\n",
    "    # build RNN model\n",
    "    model = build_rnn_regression_model(shapes)\n",
    "    \n",
    "    #logdir = './tensorboard/rnn_demo'  # if on Windows\n",
    "    logdir = '/tmp/tensorboard/rnn_demo'  # if on Unix\n",
    "    try:\n",
    "        os.makedirs(logdir)\n",
    "    except os.error:\n",
    "        pass\n",
    "    # If you want to see the plots, run tensorboard:\n",
    "    # $ tensorboard --logdir=[your_logdir]\n",
    "    #\n",
    "    # If you use SCC, you can forward the 6006 port from the cluster \n",
    "    # to your local machine via:\n",
    "    # $ ssh [SCC_cluster_name] -L 6006:localhost:6006\n",
    "    time_now = datetime.datetime.now().strftime(\"%d-%b-%H-%M-%S\")\n",
    "    run_name = 'hidden=%d_noise=%.2f' % (n_hidden_dim, noise_level)\n",
    "    sum_path = os.path.join(logdir, run_name + '_' + time_now)\n",
    "    print(sum_path)\n",
    "    max_iter_i = 0\n",
    "    with model['graph'].as_default() as g, tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sum_writer = tf.summary.FileWriter(sum_path, g)\n",
    "        for epoch_i in range(50):\n",
    "            loss_val, w_dist, h0_dist, iter_i = None, None, None, None\n",
    "            for iter_i, data_batch in enumerate(batched_data):\n",
    "                max_iter_i = max(iter_i, max_iter_i)\n",
    "                global_step = epoch_i*max_iter_i+iter_i\n",
    "                \n",
    "                # run training step\n",
    "                train_feed_dict = dict(zip(model['inputs'], data_batch))\n",
    "                to_compute = [model['train_op'], model['summ'], model['loss'],\n",
    "                              model['weights']['w'], model['weights']['h_0']]\n",
    "                _, summ, loss_val, w_val, h0_val = sess.run(to_compute, train_feed_dict)\n",
    "                \n",
    "                # compute reconstruction error\n",
    "                w_err = np.linalg.norm(w_true-w_val)\n",
    "                h0_err = np.linalg.norm(h0_true-h0_val)\n",
    "                \n",
    "                # for tensorboard\n",
    "                sum_writer.add_summary(summ, global_step)\n",
    "                sum_writer.add_summary(tf.Summary(value=[\n",
    "                    tf.Summary.Value(tag=\"w_true_dist_%dd\" % n_hidden_dim,\n",
    "                                     simple_value=w_err),\n",
    "                ]), global_step)\n",
    "                sum_writer.add_summary(tf.Summary(value=[\n",
    "                    tf.Summary.Value(tag=\"h_true_dist_%dd\" % n_hidden_dim,\n",
    "                                     simple_value=h0_err),\n",
    "                ]), global_step)\n",
    "                sum_writer.flush()\n",
    "                \n",
    "            print('epoch %d, loss=%g, w_err=%g'%(epoch_i, loss_val, w_err))\n",
    "            if global_step > 200: \n",
    "                break  # just train for 200 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.\n",
    "Now test the RNN wth varying noise levels and hidden dimensionalities.\n",
    "- For each combination of `n_hidden_dim` and `noise_level`, report the reconstruction error (`w_err`).\n",
    "- Describe how the hidden dimentionality and the noise level influence reconstruction quality. And briefly explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Experimenting with different data noise levels and hidden dimentionalities\n",
    "# We are lucky to know the true hidden dimentionality in our simultaion\n",
    "for n_hidden_dim in [10, 100, 1000]:\n",
    "    for noise_level in [0, 0.1, 0.5]:\n",
    "        print(n_hidden_dim, noise_level)\n",
    "        train_rnn_with_noise(noise_level, n_hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: LSTM implementation\n",
    "\n",
    "(40 points)\n",
    "\n",
    "Now let's attempt to recover the weights in dynamical system simulated with an LSTM.  Although LSTMs are already implemented in TensorFlow ([tutorial here](https://www.tensorflow.org/tutorials/recurrent)) ([source here](https://github.com/tensorflow/tensorflow/blob/efe5376f3dec8fcc2bf3299a4ff4df6ad3591c88/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py#L264)), you will be implementing a simple LSTM from scratch using Tensorflow in this part. \n",
    "\n",
    "\n",
    "### Q2. Implement an LSTM \n",
    "Implement an LSTM model in an analogous way,  in functions `build_lstm_regression_model()` and `train_lstm_with_noise()` below.\n",
    "The RNN regression implementation above should give you some ideas.\n",
    "We already provided an LSTM version of the dynamical system generator in the dataset generator code.\n",
    "\n",
    "- Specifically, you can follow and implement Eq. 1-6 from [this link](http://deeplearning.net/tutorial/lstm.html) in `build_lstm_regression_model()`. \n",
    "You may simplify your code by concatenating $x$ and $h$.\n",
    "- Afterwards, implement `train_lstm_with_noise()` to train the LSTM and recover the parameters. Compute the reconstruction errors for $W_c$ and $U_c$, which are the parameters used in Eq. 2 in the link.\n",
    "- For each combination of hidden dimension and noise level, report the reconstruction error (`w_err`, `u_err`) you get from the LSTM.\n",
    "\n",
    "Note:\n",
    "- The weights might not get reconstructed correctly in the LSTM case even without noise. (Why?)\n",
    "Nevertheless, the loss should decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_lstm_regression_model(shape):\n",
    "    # shape is dict with keys:\n",
    "    # n_steps_per_batch, n_hidden_dim, n_input_dim\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # inputs\n",
    "        X = tf.placeholder(tf.float32,\n",
    "                           [None, shape['n_steps_per_batch'], shape['n_input_dim']])\n",
    "        # observed outputs\n",
    "        y = tf.placeholder(tf.float32, [None, shape['n_hidden_dim']])\n",
    "        \n",
    "        #################################################################\n",
    "        ####################  PUT YOUR CODE HERE   ######################\n",
    "        # define the parameters in the LSTM (scope: weights)\n",
    "        # and carry out the computation according to Eq. 1-6 in the link\n",
    "        with tf.variable_scope('weights'):\n",
    "            pass\n",
    "        output = None  # put your result in variable 'output'\n",
    "        #################################################################\n",
    "        \n",
    "        # loss and train_op\n",
    "        loss = tf.nn.l2_loss(output - y, name='loss')\n",
    "        train_op = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "        summ = tf.summary.scalar('loss_sum_%dd' % shape['n_hidden_dim'], loss)\n",
    "\n",
    "    return {'inputs': [X, y], 'loss': loss, 'train_op': train_op, 'summ': summ,\n",
    "            'weights': {'w_c': w_c, 'u_c': u_c}, \n",
    "            'graph': g}\n",
    "    \n",
    "    \n",
    "def train_lstm_with_noise(noise_level, n_hidden_dim):\n",
    "    # generate data and random weights\n",
    "    shapes = dict(n_hidden_dim=n_hidden_dim, n_input_dim=15, n_steps_per_batch=100)\n",
    "    weights, batched_data = data_generator.build_dataset('lstm', noise=noise_level, **shapes)\n",
    "    w_c, u_c = weights[3], weights[7]  # the \"true\" weights to recover: Wc & Uc (in Eq.2)\n",
    "    \n",
    "    # this is the function you implemented\n",
    "    model = build_lstm_regression_model(shapes)\n",
    "    \n",
    "    #logdir = './tensorboard/lstm_demo'  # if on Windows\n",
    "    logdir = '/tmp/tensorboard/lstm_demo'  # if on Unix\n",
    "    try:\n",
    "        os.makedirs(logdir)\n",
    "    except os.error:\n",
    "        pass\n",
    "    time_now = datetime.datetime.now().strftime(\"%d-%b-%H-%M-%S\")\n",
    "    run_name = 'hidden=%d_noise=%.2f' % (n_hidden_dim, noise_level)\n",
    "    sum_path = os.path.join(logdir, run_name + '_' + time_now)\n",
    "    print(sum_path)\n",
    "    \n",
    "    max_iter_i = 0\n",
    "    with model['graph'].as_default() as g, tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sum_writer = tf.summary.FileWriter(sum_path, g)\n",
    "        for epoch_i in range(10):  # 10 epochs by default, feel free to change\n",
    "            loss_val, w_err, u_err, iter_i = None, None, None, None\n",
    "            for iter_i, data_batch in enumerate(batched_data):\n",
    "                max_iter_i = max(iter_i, max_iter_i)\n",
    "                global_step = epoch_i*max_iter_i+iter_i\n",
    "                \n",
    "                ###############################################################\n",
    "                ###################   PUT YOUR CODE HERE   ####################\n",
    "                train_feed_dict = None  # define your train_feed_dict\n",
    "                to_compute = []  # the things to compute, including at least:\n",
    "                                 # loss_val, w_err, u_err\n",
    "                outputs = sess.run(to_compute, train_feed_dict)\n",
    "                w_err = None  # compute them and report\n",
    "                u_err = None\n",
    "                # [optional] you can also do early stopping, e.g. 300 iterations\n",
    "                # if global_step > 300:\n",
    "                #    break\n",
    "                # use sum_writer for tensorboard, see train_rnn_with_noise()\n",
    "                sum_writer.add_summary(...)\n",
    "                sum_writer.flush()\n",
    "                ###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n_hidden_dim in [10, 100, 1000]:\n",
    "    for noise_level in [0, 0.1, 0.5]:\n",
    "        print(n_hidden_dim, noise_level)\n",
    "        train_lstm_with_noise(noise_level, n_hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Neural Tolstoy Model (a character-prediction LSTM)\n",
    "\n",
    "(20 points)\n",
    "\n",
    "In this part you will train a character prediction LSTM that predicts the next character given previous characters. \n",
    "The training data is from the book \"War and Peace\" by Leo Tolstoy. \n",
    "\n",
    "### Q3.\n",
    "Compared to the LSTM you implemented in the previous part, the main difference in the character prediction LSTM is that it predicts *discrete* outputs (characters, or their indices in the vocabulary), therefore it is a classification model. As you may recall, the usual choice of loss function for classification is softmax + cross entropy.\n",
    "\n",
    "We have already provided functions for loading the training data. Please define your discrete LSTM in `build_lstm_discrete_prediction_model()`.\n",
    "\n",
    "Hints: \n",
    "- Your LSTM should predict a single character at a time. (That's why it's called a character prediction LSTM.) We are not considering  many-to-many LSTMs here.\n",
    "- Feeding the previous input into the LSTM should help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tolstoy_reader\n",
    "\n",
    "def get_default_gpu_session(fraction=0.333):\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = fraction\n",
    "    return tf.Session(config=config)\n",
    "\n",
    "def run_tolstoy_train(n_hid):\n",
    "    # generate data\n",
    "    btg, map_dict, backmap_dict = \\\n",
    "        tolstoy_reader.batch_tolstoy_generator(batch_size=200, seq_size=100)\n",
    "    shape = dict(n_steps_per_batch=100, n_unique_ids=len(map_dict), n_hidden_dim=n_hid)\n",
    "    # define LSTM\n",
    "    model = build_lstm_discrete_prediction_model(shape)\n",
    "    \n",
    "    #logdir = './tensorboard/tolstoy'  # if on Windows\n",
    "    logdir = '/tmp/tensorboard/tolstoy'  # if on Unix\n",
    "    try:\n",
    "        os.makedirs(logdir)\n",
    "    except os.error:\n",
    "        pass\n",
    "    time_now = datetime.datetime.now().strftime(\"%d-%b-%H-%M-%S\")\n",
    "    run_name = 'hidden=%d' % n_hid\n",
    "    sum_path = os.path.join(logdir, run_name + '_' + time_now)\n",
    "    print(sum_path)\n",
    "    max_iter_i = 0\n",
    "    with model['graph'].as_default() as g, get_default_gpu_session(0.9) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sum_writer = tf.summary.FileWriter(sum_path, g)\n",
    "        for epoch_i in range(10):\n",
    "            for iter_i, data_batch in enumerate(btg):\n",
    "                max_iter_i = max(iter_i, max_iter_i)\n",
    "                global_step = epoch_i*max_iter_i+iter_i\n",
    "                \n",
    "                # run training step\n",
    "                train_feed_dict = dict(zip(model['inputs'], data_batch))\n",
    "                to_compute = [model['train_op'], model['summ'], model['loss']]\n",
    "                _, summ, loss_val = sess.run(to_compute, train_feed_dict)\n",
    "                \n",
    "                # for tensorboard\n",
    "                sum_writer.add_summary(summ, global_step)\n",
    "                sum_writer.flush()\n",
    "                \n",
    "                # display loss\n",
    "                if iter_i % 100 == 0:\n",
    "                    print(loss_val, end=', ')\n",
    "                if iter_i % 1000:\n",
    "                    continue\n",
    "                # test generation\n",
    "                pred_length = 50\n",
    "                data_input = next(iter(btg))[0][[0]]\n",
    "                original_sample = data_input.copy()\n",
    "                pred_seq = []\n",
    "                for _ in range(pred_length):\n",
    "                    pred = sess.run(model['pred'], {model['inputs'][0]: data_input})\n",
    "                    pred_seq.append(pred[0])\n",
    "                    data_input = np.roll(data_input, -1, axis=1)\n",
    "                    data_input[0, -1] = pred[0]\n",
    "                print('[%d] Input text:' % (iter_i))\n",
    "                print(''.join([backmap_dict[x] for x in original_sample[0]]))\n",
    "                print('[%d] Generated continuation:' % (iter_i))\n",
    "                print(''.join([backmap_dict[x] for x in pred_seq]))\n",
    "                print(pred_seq)\n",
    "                print()\n",
    "                \n",
    "def build_lstm_discrete_prediction_model(shape):\n",
    "    # shape is dict with keys:\n",
    "    # n_steps_per_batch, n_unique_ids, n_hidden_dim\n",
    "    with tf.Graph().as_default() as g:\n",
    "        X = tf.placeholder(tf.int64, [None, shape['n_steps_per_batch']])\n",
    "        y = tf.placeholder(tf.int64, [None])\n",
    "        \n",
    "        ################################################################\n",
    "        ####################   PUT YOUR CODE HERE   ####################\n",
    "        # define LSTM parameters (scope: weights)\n",
    "        with tf.variable_scope('weights'):\n",
    "            pass\n",
    "        logits = None  # compute logits for each discrete output\n",
    "        loss = None  # compute loss with respect to (logits, y)\n",
    "        ################################################################\n",
    "\n",
    "        # pred, train_op\n",
    "        pred = tf.argmax(logits, axis=1)\n",
    "        train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "        summ = tf.summary.scalar('loss_summ', loss)\n",
    "\n",
    "    return {'inputs': [X, y], 'loss': loss, 'train_op': train_op, 'summ': summ,\n",
    "            'graph': g, 'pred': pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the LSTM and see what it says! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden = 200\n",
    "run_tolstoy_train(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multi-modal Restricted Boltzmann Machines\n",
    "\n",
    "(30 points)\n",
    "\n",
    "- Relevant reading: Goodfellow chapter 20 through 20.8, in particular 20.2 and 20.3. Also see the [lecture notes](https://drive.google.com/file/d/0B8xkaMshuaF9dFVRclo3RENKdUk/view?usp=sharing).\n",
    "\n",
    "For this question, we will consider designing an unsupervised probability distribution  over two modalities.  Suppose we have data consisting of images and their associated captions, so that each input data point may be thought of as the pair $(t,v)$, where $t = (t_1, ..., t_n)$ is the text data and $v = (v_1, ..., v_m)$ is the visual data.  For simplicity, we will consider both as vector inputs (so, for instance, we will not be considering any two-dimensional convolutions for the visual data); the main difference between the two inputs is that the text input is discrete-valued, where we assume that each $t_i$ can take on $k$ different values, and each visual input is real-valued.\n",
    "\n",
    "Following our discussion of Restricted Boltzmann Machines (RBMs), we will further consider a hidden layer with **binary** hidden units $(h_1, ..., h_d)$, and define a joint probability distribution over an input data point and a hidden layer.  In particular, let us define two matrices $W_t\\in\\mathbb{R}^{n\\times d}$ and $W_v\\in\\mathbb{R}^{m\\times d}$; these matrices correspond to weights between text input and hidden units, and weights between visual input and hidden units, respectively.  In addition, we have the bias vectors $a$, $b$, and $c$, associated with the text, visual, and hidden units, respectively.\n",
    "The multi-modal RBM is illustrated below.\n",
    "\n",
    "<img src=\"MultimodalRBM.png\" width=\"550\">\n",
    "\n",
    "Next we define the joint probability distribution $p(t,v,h)$ over text inputs, visual inputs, and hidden states.  First we let the energy function be: \n",
    "\n",
    "$E(v,t,h) = -t^T W_t h - v^T W_v h - a^T t + (0.5 t^Tt - b^T v + 0.5 v^T v) - c^T h$.\n",
    "\n",
    "Then let the joint probability be defined as:\n",
    "\n",
    "$p(v,t,h) = \\frac{1}{Z}exp(-E(v,t,h)),$\n",
    "\n",
    "where $Z$ is the normalizing constant that ensures that the probability distribution is properly normalized to sum/integrate to one.\n",
    "\n",
    "Our goal is to perform maximum likelihood estimation for the parameters $W_t$, $W_v$, and $c$ (we will assume $a$ and $b$ are fixed and known), given a set of $N$ data points $D = \\{(t^{(1)},v^{(1)}), ..., (t^{(N)},v^{(N)})\\}$.  As is typical in such settings, we assume that the data in $D$ are i.i.d. samples.\n",
    "\n",
    "Please write your solutions in the cells below, or hand in a hard copy.\n",
    "For each question you must show your work to receive full credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.1 \n",
    "\n",
    "Write down the marginal probability for a single data point $(t^{(1)}, v^{(1)})$, namely $p(t^{(1)}, v^{(1)})$. Treating this probability as the likelihood for a single point, write down the log-likelihood for the entire data set $D$ as a function of the parameters $(W_t, W_v,  c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Put your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.2 \n",
    "\n",
    "Write down the partial derivative of $E(t,v,h)$ with respect to each of the parameters, i.e., each entry of $W_t$, each entry of $W_v$, and each entry of $c$. You can also directly write in matrix/vector forms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Put your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.3 \n",
    "\n",
    "Derive the conditional distributions $p(h | v, t)$, $p(v | h)$ and $p(t | h)$, either in elementwise form or in vector form.\n",
    "Also establish that $v$ and $t$ are conditionally independent given $h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Put your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.4\n",
    "\n",
    "Write down the partial derivative of the log-likelihood for $D$ with respect to each of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Put your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q4.5 \n",
    "\n",
    "Extend the contrastive divergence approach for standard RBMs, as discussed in class, to suggest a gradient ascent procedure for learning the parameters of our multi-modal RBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Put your answer here]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
